# 寻找一种易于理解的一致性算法（扩展版）

说明: 克隆自 https://github.com/maemual/raft-zh_cn, 因不太喜欢部分中文翻译，还是喜欢保持英文, 如 leader, follower, candidate; 还有一些地方同时保留英文和中文；

## 摘要

Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如leader选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。从一个用户研究的结果可以证明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。

## 1 介绍

一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。正因为如此，一致性算法在构建可信赖的大规模软件系统中扮演着重要的角色。在过去的 10 年里，Paxos  算法统治着一致性算法这一领域：绝大多数的实现都是基于 Paxos 或者受其影响。同时 Paxos 也成为了教学领域里讲解一致性问题时的示例。

但是不幸的是，尽管有很多工作都在尝试降低它的复杂性，但是 Paxos 算法依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。

和 Paxos 算法进行过努力之后，我们开始寻找一种新的一致性算法，可以为构建实际的系统和教学提供更好的基础。我们的做法是不寻常的，我们的首要目标是可理解性：我们是否可以在实际系统中定义一个一致性算法，并且能够比 Paxos 算法以一种更加容易的方式来学习。此外，我们希望该算法方便系统构建者的直觉的发展。不仅一个算法能够工作很重要，而且能够显而易见的知道为什么能工作也很重要。

Raft 一致性算法就是这些工作的结果。在设计 Raft 算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（Raft 主要被分成了leader选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。一份针对两所大学 43 个学生的研究表明 Raft 明显比 Paxos 算法更加容易理解。在这些学生同时学习了这两种算法之后，和 Paxos 比起来，其中 33 个学生能够回答有关于 Raft 的问题。

Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性：

* **强leader**：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，log 只从leader发送给其他的服务器。这种方式简化了对复制log的管理并且使得 Raft 算法更加易于理解。
* **leader选举**：Raft 算法使用一个随机计时器来选举leader。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。
* **成员关系调整**：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。

我们相信，Raft 算法不论出于教学目的还是作为实践项目的基础都是要比 Paxos 或者其他一致性算法要优异的。它比其他算法更加简单，更加容易理解；它的算法描述足以实现一个现实的系统；它有好多开源的实现并且在很多公司里使用；它的安全性已经被证明；它的效率和其他算法比起来也不相上下。

接下来，这篇论文会介绍以下内容：复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），讨论我们为了可理解性而采取的方法（第 4 节），阐述 Raft 一致性算法（第 5-8 节），评价 Raft 算法（第 9 节），以及一些相关的工作（第 10 节）。

## 2 复制状态机

一致性算法是从复制状态机的背景下提出的（参考英文原文引用37）。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群leader，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的的复制状态机去管理leader选举和存储配置信息并且在leader宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。

![图 1 ](./images/raft-图1.png)

> 图 1 ：复制状态机的结构。一致性算法管理着来自client指令的复制log。状态机从log中处理相同顺序的相同指令，所以产生的结果也是相同的。

复制状态机通常都是基于复制log实现的，如图 1。每一个服务器存储一个包含一系列指令的log，并且按照log的顺序进行执行。每一个log都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。

保证复制log相同就是一致性算法的工作了。在一台服务器上，一致性模块接收client发送来的指令然后增加到自己的log中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的log最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照log顺序处理他们，然后输出结果被返回给client。因此，服务器集群看起来形成一个高可靠的状态机。

实际系统中使用的一致性算法通常含有以下特性：

* 安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证正确。
* 可用性：集群中只要有大多数的机器可运行并且能够相互通信、和client通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。他们当有稳定的存储的时候可以从状态中恢复回来并重新加入集群。
* 不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。
* 通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。

## 3 Paxos 算法的问题

在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制log。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。Paxos 保证安全性和活性，同时也支持集群成员关系的变更。Paxos 的正确性已经被证明，在通常情况下也很高效。

不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。

我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份log而不是单一的log记录）能够被分解成其他的方式并且更加直接和明显。

Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。

而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立的选择一组log 然后合并成一个序列化的log并没有带来太多的好处，仅仅增加了不少复杂性。围绕着log来设计一个系统是更加简单高效的；新log 以严格限制的顺序增添到log列表中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱leader的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个leader，然后让他去协调所有的决议，会更加简单快速。

因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型：

> 在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。

由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft 算法就是这次实验的结果。

## 4 为了可理解性的设计

设计 Raft 算法我们有几个初衷：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在大多数的情况下都是可用的；并且它的大部分操作必须是高效的。但是我们最重要也是最大的挑战是可理解性。它必须保证对于普遍的人群都可以十分容易的去理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行必然的扩展。

在设计 Raft 算法的时候，有很多的点需要我们在各种备选方案中进行选择。在这种情况下，我们评估备选方案基于可理解性原则：解释各个备选方案有多大的难度（例如，Raft 的状态空间有多复杂，是否有微妙的暗示）？对于一个读者而言，完全理解这个方案和暗示是否容易？

我们意识到对这种可理解性分析上具有高度的主观性；尽管如此，我们使用了两种通常适用的技术来解决这个问题。第一个技术就是众所周知的问题分解：只要有可能，我们就将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成leader选举，log复制，安全性和角色改变几个部分。

我们使用的第二个方法是通过减少状态的数量来简化需要考虑的状态空间，使得系统更加连贯并且在可能的时候消除不确定性。特别的，所有的log是不允许有空洞的，并且 Raft 限制了log之间变成不一致状态的可能。尽管在大多数情况下我们都试图去消除不确定性，但是也有一些情况下不确定性可以提升可理解性。尤其是，随机化方法增加了不确定性，但是他们有利于减少状态空间数量，通过处理所有可能选择时使用相似的方法。我们使用随机化去简化 Raft 中leader选举算法。

## 5 Raft 一致性算法

Raft 是一种用来管理章节 2 中描述的复制log的算法。图 2 为了参考之用，总结这个算法的简略版本，图 3 列举了这个算法的一些关键特性。图中的这些元素会在剩下的章节逐一介绍。

Raft 通过选举一个高贵的leader，然后给予他全部的管理复制log的责任来实现一致性。leader从client接收log ，把log 复制到其他服务器上，并且当保证安全性的时候告诉其他的服务器应用log 到他们的状态机中。拥有一个leader大大简化了对复制log的管理。例如，leader可以决定新的log 需要放在log列表中的什么位置而不需要和其他服务器商议，并且数据都从leader流向其他服务器。一个leader可以宕机，可以和其他服务器失去连接，这时一个新的leader会被选举出来。

通过leader的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题会在接下来的子章节中进行讨论：

* **leader选举**：一个新的leader需要被选举出来，当现存的leader宕机的时候（章节 5.2）
* **log复制**：leader必须从client接收log然后复制到集群中的其他节点，并且强制要求其他节点的log保持和自己相同。
* **安全性**：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的log 到它的状态机中，那么其他服务器节点不能在同一个log索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到一个额外的选举机制（5.2 节）上的限制。

在展示一致性算法之后，这一章节会讨论可用性的一些问题和计时在系统的作用。

**状态**：

|状态|所有服务器上持久存在的|
|-------|------|
|currentTerm | 服务器最后一次知道的任期号（初始化为 0，持续递增）|
|votedFor | 在当前获得选票的candidate的 Id|
| log[] | log 列表；每一个log包含一个用户状态机执行的指令，和收到时的任期号 |

|状态|所有服务器上经常变的|
|-------|------|
| commitIndex| 已知的最大的已经被提交的log 的索引值|
| lastApplied| 最后被应用到状态机的log 索引值（初始化为 0，持续递增）|

| 状态 | 在leader里经常改变的 （选举后重新初始化） |
|----|--------|
| nextIndex[] | 对于每一个服务器，需要发送给他的下一个log 的索引值（初始化为leader最后索引值加一） |
| matchIndex[] | 对于每一个服务器，已经复制给他的log的最高索引值|


** AppendEntries RPC**：

由leader负责调用来复制log指令；也会用作heartbeat

| 参数 | 解释 |
|----|----|
|term| leader的任期号 |
|leaderId| leader的 Id，以便于follower重定向请求 |
|prevLogIndex|新的log 紧随之前的索引值|
|prevLogTerm|prevLogIndex log的任期号|
|entries[]|准备存储的log （表示心跳时为空；一次性发送多个是为了提高效率）|
|leaderCommit|leader已经提交的log的索引值|

| 返回值| 解释|
|---|---|
|term|当前的任期号，用于leader去更新自己|
|success|follower包含了匹配上 prevLogIndex 和 prevLogTerm 的log时为true|

接收者实现：

1. 如果 `term < currentTerm` 就返回 false （5.1 节）
2. 如果在 prevLogIndex 位置处的log 的任期号和 prevLogTerm 不匹配，则返回 false （5.3 节）
3. 如果已经存在的log 和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 （5.3 节）
4. 附加log中尚未存在的任何新log
5. 如果 `leaderCommit > commitIndex`，令 commitIndex 等于 leaderCommit 和 新log 索引值中较小的一个

**RequestVote RPC**：

由candidate负责调用用来征集选票（5.2 节）

| 参数 | 解释|
|---|---|
|term| candidate的任期号|
|candidateId| 请求选票的candidate的 Id |
|lastLogIndex| candidate的最后log 的索引值|
|lastLogTerm| candidate最后log 的任期号|

| 返回值| 解释|
|---|---|
|term| 当前任期号，以便于candidate去更新自己的任期号|
|voteGranted| candidate赢得了此张选票时为true|

接收者实现：

1. 如果`term < currentTerm`返回 false （5.2 节）
2. 如果 votedFor 为空或者为 candidateId，并且candidate的log至少和自己一样新，那么就投票给他（5.2 节，5.4 节）

**所有服务器需遵守的规则**：

所有服务器：

* 如果`commitIndex > lastApplied`，那么就 lastApplied 加一，并把`log[lastApplied]`应用到状态机中（5.3 节）
* 如果接收到的 RPC 请求或响应中，任期号`T > currentTerm`，那么就令 currentTerm 等于 T，并切换状态为follower（5.1 节）

follower（5.2 节）：

* 响应来自candidate和leader的请求
* 如果在超过选举超时时间的情况之前没有收到**当前leader**（即该leader的任期需与这个follower的当前任期相同）的心跳/附加log，或者是给某个candidate投了票，就自己变成candidate

candidate（5.2 节）：

* 在转变成candidate后就立即开始选举过程
	* 自增当前的任期号（currentTerm）
	* 给自己投票
	* 重置选举超时计时器
	* 发送 RequestVote RPC 给其他所有服务器
* 如果接收到大多数服务器的选票，那么就变成leader
* 如果接收到来自新的leader的 AppendEntries RPC，转变成follower
* 如果选举过程超时，再次发起一轮选举

leader：

* 一旦成为leader：发送空的 AppendEntries RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止follower超时（5.2 节）
*  如果接收到来自client的请求：附加log到本地log列表中，在log被应用到状态机后响应client（5.3 节）
*  如果对于一个follower，最后log 的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有log ：
	* 如果成功：更新相应follower的 nextIndex 和 matchIndex
	* 如果因为log不一致而失败，减少 nextIndex 重试
* 如果存在一个满足`N > commitIndex`的 N，并且大多数的`matchIndex[i] ≥ N`成立，并且`log[N].term == currentTerm`成立，那么令 commitIndex 等于这个 N （5.3 和 5.4 节）

![图 2 ](./images/raft-图2.png)

> 图 2：一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和log压缩）。

| 特性|特性(EN)| 解释|
|---|---|---|
|选举安全特性 | Election Safety | 对于一个给定的任期号，最多只会有一个leader被选举出来（5.2 节） |
|leader只附加原则 | Leader Append-Only | leader绝对不会删除或者覆盖自己的log，只会增加（5.3 节） |
|log匹配原则 | Log Matching | 如果两个log列表在相同的索引位置的log 的任期号相同，那么我们就认为这个log从头到这个索引位置之间全部完全相同（5.3 节） |
|leader完全特性 | Leader Completedness |如果某个log 在某个任期号中已经被提交，那么这个log必然出现在更大任期号的所有leader中（5.4 节）|
|状态机安全特性 | State Machine Safety | 如果一个leader已经将给定的索引值位置的log 应用到状态机中，那么其他任何的服务器在这个索引位置不会应用一个不同的log（5.4.3 节） |

![图 3 ](./images/raft-图3.png)

> 图 3：Raft 在任何时候都保证以上的各个特性。


### 5.1 Raft 基础

一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：leader、follower或者candidate。在通常情况下，系统中只有一个leader并且其他的节点全部都是follower。follower都是被动的：他们不会发送任何请求，只是简单的响应来自leader或者candidate的请求。leader处理所有的client请求（如果一个client和follower联系，那么follower会把请求重定向给leader）。第三种状态，candidate，是用来在 5.2 节描述的选举新leader时使用。图 4 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。

![图 4 ](./images/raft-图4.png)

> 图 4：服务器状态。follower只响应来自其他服务器的请求。如果follower接收不到消息，那么他就会变成candidate并发起一次选举。获得集群中大多数选票的candidate将成为leader。在一个任期内，leader一直都会是leader直到自己宕机了。

![图 5](./images/raft-图5.png)

> 图 5：时间被划分成一个个的任期，每个任期开始都是一次选举。在选举成功后，leader会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有leader而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。

Raft 把时间分割成任意长度的**任期**，如图 5。任期用连续的整数标记。每一段任期从一次**选举**开始，就像章节 5.2 描述的一样，一个或者多个candidate尝试成为leader。如果一个candidate赢得选举，然后他就在接下来的任期内充当leader的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有leader结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个leader。

不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如陈旧的leader。每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。如果一个candidate或者leader发现自己的任期号过期了，那么他会立即恢复成follower状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。

Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。RequestVote RPC 由candidate在选举期间发起（章节  5.2），然后 AppendEntries RPC 由leader发起，用来复制log和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPCs 来获得最佳的性能。

### 5.2 leader选举

Raft 使用一种心跳机制来触发leader选举。当服务器程序启动时，他们都是follower身份。一个服务器节点继续保持着follower状态只要他从leader或者候选者处接收到有效的 RPCs。leader周期性的向所有follower发送心跳包（即不包含log内容的 AppendEntries RPC）来维持自己的权威。如果一个follower在一段时间里没有接收到任何消息，也就是**选举超时**，那么他就会认为系统中没有可用的leader,并且发起选举以选出新的leader。

要开始一次选举过程，follower先要增加自己的当前任期号并且转换到candidate状态。然后他会并行的向集群中的其他服务器节点发送 RequestVote RPC 来给自己投票。candidate会继续保持着当前状态直到以下三件事情之一发生：

1. 他自己赢得了这次的选举，
2. 其他的服务器成为leader，
3. 一段时间之后没有任何一个获胜的人。

这些结果会分别的在下面的段落里进行讨论。

当一个candidate从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为leader。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：5.4 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个candidate赢得此次选举（图 3 中的选举安全性）。一旦candidate赢得选举，他就立即成为leader。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的leader的产生。

在等待投票的时候，candidate可能会从其他的服务器接收到声明它是leader的 AppendEntries RPC。如果这个leader的任期号（包含在此次的 RPC中）不小于candidate当前的任期号，那么candidate会承认leader合法并回到follower状态。 如果此次 RPC 中的任期号比自己小，那么candidate就会拒绝这次的 RPC 并且继续保持candidate状态。

第三种可能的结果是candidate既没有赢得选举也没有输：如果有多个follower同时成为candidate，那么选票可能会被瓜分以至于没有candidate可以赢得大多数人的支持。当这种情况发生的时候，每一个candidate都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。

Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个candidate在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。9.3 节展示了这种方案能够快速的选出一个leader。

leader选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个candidate都被赋予一个唯一的排名，供candidate之间竞争时进行选择。如果一个candidate发现另一个candidate拥有更高的排名，那么他就会回到follower状态，这样高排名的candidate能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入candidate状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。

### 5.3 log复制

一旦一个leader被选举出来，他就开始为client提供服务。client的每一个请求都包含一条被复制状态机执行的指令。leader把这条指令作为一条新的log 附加到log列表中去，然后并行的发起 AppendEntries RPC 给其他的服务器，让他们复制这条log。当这条log被安全的复制（下面会介绍），leader会应用这条log到它的状态机中然后把执行的结果返回给client。如果follower崩溃或者运行缓慢，再或者网络丢包，leader会不断的重复尝试附加log  RPCs （尽管已经回复了client）直到所有的follower都最终存储了所有的log 。

![图 6](./images/raft-图6.png)

> 图 6：log列表由有序号标记的log项组成。每个log都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个log当可以安全的被应用到状态机中去的时候，就认为是可以提交了。

log列表以图 6 展示的方式组织。每一个log 存储一条状态机指令和从leader收到这条指令时的任期号。log中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3 中的某些性质。每一条log 同时也都有一个整数索引值来表明它在log中的位置。

leader来决定什么时候把log 应用到状态机中是安全的；这种log 状态被称为**已提交**。Raft 算法保证所有已提交的log 都是持久化的并且最终会被所有可用的状态机执行。在leader将创建的log 复制到大多数的服务器上的时候，log 就会被提交（例如在图 6 中的log 7）。同时，leader的log列表中之前的所有log 也都会被提交，包括由其他leader创建的log。5.4 节会讨论某些当在leader改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。leader跟踪了最大的将会被提交的log的索引，并且索引值会被包含在未来的所有 AppendEntries RPC （包括心跳包），这样其他的服务器才能最终知道leader的提交位置。一旦follower知道一条log 已经被提交，那么他也会将这个log 应用到本地的状态机中（按照log的顺序）。

我们设计了 Raft 的log机制来维护一个不同服务器的log之间的高层次的一致性。这么做不仅简化了系统的行为也使得更加可预计，同时他也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些同时也组成了图 3 中的日志匹配特性：

* 如果在不同的log列表中的两个log拥有相同的索引和任期号，那么他们存储了相同的指令。
* 如果在不同的log列表中的两个log拥有相同的索引和任期号，那么他们之前的所有log 也全部相同。

第一个特性来自这样的一个事实，leader最多在一个任期里在指定的一个log索引位置创建一条log ，同时log 在log列表中的位置也从来不会改变。第二个特性由 AppendEntries RPC 的一个简单的一致性检查所保证。在发送 AppendEntries RPC 的时候，leader会把新的log 紧接着之前的log的索引位置和任期号包含在里面。如果follower在它的log列表中找不到包含相同索引位置和任期号的log，那么他就会拒绝接收新的log 。一致性检查就像一个归纳步骤：一开始空的log状态肯定是满足日志匹配特性的，然后当log扩展的时候一致性检查保护了日志匹配特性。因此，每当 AppendEntries RPC 返回 true 时，leader就知道follower的log一定是和自己相同的了。


在正常的操作中，leader和follower的log保持一致性，所以 AppendEntries RPC 的一致性检查从来不会失败。然而，leader崩溃的情况会使得log处于不一致的状态（老的leader可能还没有完全复制所有的log ）。这种不一致问题会在leader和follower的一系列崩溃下加剧。图 7 展示了follower的log可能和新的leader不同的方式。follower可能会丢失一些在新的leader中有的log ，他也可能拥有一些leader没有的log ，或者两者都发生。丢失或者多出log 可能会持续多个任期。

![图 7](./images/raft-图7.png)

> 图 7：当一个leader成功当选时，follower可能是任何情况（a-f）。每一个盒子表示是一个log ；里面的数字表示任期号。follower可能会缺少一些log （a-b），可能会有一些未被提交的log （c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是leader，已附加了一些log 到自己的log列表中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为leader，并且又增加了一些log 到自己的log列表中；在任期 2 和任期 3 的log被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。


在 Raft 算法中，leader处理不一致是通过强制follower直接复制自己的log来解决了。这意味着在follower中的冲突的log 会被leader的log覆盖。5.4 节会阐述如何通过增加一些限制来使得这样的操作是安全的。

要使得follower的log进入和自己一致的状态，leader必须找到最后两者达成一致的地方，然后删除从那个点之后的所有log ，发送自己的log给follower。所有的这些操作都在进行 AppendEntries RPC 的一致性检查时完成。leader针对每一个follower维护了一个 **nextIndex**，这表示下一个需要发送给follower的log 的索引地址。当一个leader刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条log的 index 加 1（图 7 中的 11）。如果一个follower的log和leader不一致，那么在下一次的 AppendEntries RPC 时的一致性检查就会失败。在被follower拒绝之后，leader就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得leader和follower的log达成一致。当这种情况发生， AppendEntries RPC 就会成功，这时就会把follower冲突的log 全部删除并且加上leader的log。一旦 AppendEntries RPC 成功，那么follower的log就会和leader保持一致，并且在接下来的任期里一直继续保持。

如果需要的话，算法可以通过减少被拒绝的 AppendEntries RPC 的次数来优化。例如，当 AppendEntries RPC 的请求被拒绝的时候，follower可以包含冲突的log的任期号和自己存储的那个任期的最早的索引地址。借助这些信息，leader可以减小 nextIndex 越过所有那个任期冲突的所有log ；这样就变成每个任期需要一次 AppendEntries RPC 而不是每个log一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的log。

通过这种机制，leader在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后log就能自动的在回复 AppendEntries RPC 的一致性检查失败的时候自动趋于一致。leader从来不会覆盖或者删除自己的log（图 3 的leader只附加特性）。

log复制机制展示出了第 2 节中形容的一致性特性：Raft 能够接受，复制并应用新的log 只要大部分的机器是工作的；在通常的情况下，新的log 可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的follower不会影响整体的性能。

### 5.4 安全性

前面的章节里描述了 Raft 算法是如何选举和复制log的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个follower可能会进入不可用状态同时leader已经提交了若干的log ，然后这个follower可能会被选举为leader并且覆盖这些log ；因此，不同的状态机可能会执行不同的指令序列。

这一节通过在leader选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的leader对于给定的任期号，都拥有了之前任期的所有被提交的log （图 3 中的leader完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于leader完整特性的简要证明，并且说明leader完整性特性是如何引导复制状态机做出正确行为的。

#### 5.4.1 选举限制

在任何基于leader的一致性算法中，leader都必须存储所有已经提交的log 。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的log ，它也能被选为leader。这些算法都包含一些额外的机制来识别丢失的log 并把他们传送给新的leader，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证所有之前的任期号中已经提交的log 在选举的时候都会出现在新的leader中，不需要传送这些log 给leader。这意味着log 的传送是单向的，只从leader传给follower，并且leader从不会覆盖自身本地log中已经存在的log。

Raft 使用投票的方式来阻止一个candidate赢得选举除非这个candidate包含了所有已经提交的log 。candidate为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的log 在这些服务器节点中肯定存在于至少一个节点上。如果candidate的log至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的log 。RequestVote RPC 实现了这样的限制：RPC 中包含了candidate的log信息，然后投票人会拒绝掉那些log没有自己新的投票请求。

Raft 通过比较两份log列表中最后一条log 的索引值和任期号定义谁的log比较新。如果两份log最后的log的任期号不同，那么任期号大的log更加新。如果两份log最后的log任期号相同，那么log列表比较长的那个就更加新。

#### 5.4.2 提交之前任期内的log 

如同 5.3 节介绍的那样，leader知道一条当前任期内的log记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个leader在提交log 之前崩溃了，未来后续的leader会继续尝试复制这条log记录。然而，一个leader不能断定一个之前任期里的log 被保存到大多数服务器上的时候就一定已经提交了。图 8 展示了一种情况，一条已经被存储到大多数节点上的老log ，也依然有可能会被未来的leader覆盖掉。

![图 8](./images/raft-图8.png)

> 图 8：如图的时间序列展示了为什么leader无法决定对老任期号的log 进行提交。在 (a) 中，S1 是leader，部分的复制了索引位置 2 的log 。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从client接收了一条不一样的log 放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制log。在这时，来自任期 2 的那条log已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的log。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的log 复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的log 就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的log 就会被提交。

为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的log 。只有leader当前任期里的log 通过计算副本数目可以被提交；一旦当前任期的log 以这种方式被提交，那么由于日志匹配特性，之前的log 也都会被间接的提交。在某些情况下，leader可以安全的知道一个老的log 是否已经被提交（例如，该log是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。

当leader复制之前任期里的log时，Raft 会为所有log保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的leader要重新复制之前的任期里的log时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出log，因为它可以随着时间和log的变化对log维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新leader只需要发送更少log （其他算法中必须在他们被提交之前发送更多的冗余log 来为他们重新编号）。

#### 5.4.3 安全性论证

在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论leader完整性特性（这一讨论基于 9.2 节的安全性证明）。我们假设leader完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的leader（leader T）在任期内提交了一条log ，但是这条log没有被存储到未来某个任期的leader的log列表中。设大于 T 的最小任期 U 的leader U 没有这条log。

![图 9](./images/raft-图9.png)

> 图 9：如果 S1 （任期 T 的leader）提交了一条新的log在它的任期里，然后 S5 在之后的任期 U 里被选举为leader，然后至少会有一个机器，如 S3，既拥有来自 S1 的log，也给 S5 投票了。

1. 在leader U 选举的时候一定没有那条被提交的log （leader从不会删除或者覆盖任何log）。
2. leader T 复制这条log给集群中的大多数节点，同时，leader U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自leader T 的log ，并且给leader U 投票了，如图 9。这个投票者是产生这个矛盾的关键。
3. 这个投票者必须在给leader U 投票之前先接受了从leader T 发来的已经被提交的log ；否则他就会拒绝来自leader T 的AppendEntries 请求（因为此时他的任期号会比 T 大）。
4. 投票者在给leader U 投票时依然保存有这条log，因为任何中间的leader都包含该log （根据上述的假设），leader从不会删除log，并且follower只有在和leader冲突的时候才会删除log。
5. 投票者把自己选票投给leader U 时，leader U 的log必须和投票者自己一样新。这就导致了两者矛盾之一。
6. 首先，如果投票者和leader U 的最后一条log的任期号相同，那么leader U 的log列表至少和投票者一样长，所以leader U 的log一定包含所有投票者的log。这是另一处矛盾，因为投票者包含了那条已经被提交的log ，但是在上述的假设里，leader U 是不包含的。
7. 除此之外，leader U 的最后一条log的任期号就必须比投票者大了。此外，他也比 T 大，因为投票人的最后一条log的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的log）。创建了leader U 最后一条log的之前leader一定已经包含了那条被提交的log（根据上述假设，leader U 是第一个不包含该log 的leader）。所以，根据日志匹配特性，leader U 一定也包含那条被提交的log，这里产生矛盾。
8. 这里完成了矛盾。因此，所有比 T 大的leader一定包含了所有来自 T 的已经被提交的log。
9. log匹配原则保证了未来的leader也同时会包含被间接提交的log，例如图 8 (d) 中的索引 2。

通过leader完全特性，我们就能证明图 3 中的状态机安全特性，即如果服务器已经在某个给定的索引值应用了log 到自己的状态机里，那么其他的服务器不会应用一个不一样的log到同一个索引值上。在一个服务器应用一条log 到他自己的状态机中时，他的log必须和leader的log在该log和之前的log上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的log的最小任期；log完全特性保证拥有更高任期号的leader会存储相同的log ，所以之后的任期里应用某个索引位置的log 也会是相同的值。因此，状态机安全特性是成立的。

最后，Raft 要求服务器按照log列表中索引位置顺序应用log 。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的log序列集到自己的状态机中，并且是按照相同的顺序。

### 5.5 follower和candidate崩溃

到目前为止，我们都只关注了leader崩溃的情况。follower和candidate崩溃后的处理方式比leader要简单的多，并且他们的处理方式是相同的。如果follower或者candidate崩溃了，那么后续发送给他们的 RPC 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPC 都是幂等的，所以这样重试不会造成任何问题。例如一个follower如果收到附加log请求但是他已经包含了这一log，那么他就会直接忽略这个新的请求。

### 5.6 时间和可用性

Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应client）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，candidate将没有足够长的时间来赢得选举；没有一个稳定的leader，Raft 将无法工作。

leader选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的leader,只要系统满足下面的时间要求：

> 广播时间（broadcastTime）  <<  选举超时时间（electionTimeout） <<  平均故障间隔时间（MTBF）

在这个不等式中，广播时间指从一个服务器并行的发送 RPC 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举的超时时间限制；平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样leader才能够发送稳定的心跳消息来阻止follower开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定运行。当leader崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。

广播时间和平均故障间隔时间是由系统决定的，但选举超时时间是我们自己选择的。Raft 的 RPC 需要接收方将信息持久化保存到稳定存储中，所以广播时间大约 0.5 毫秒到 20 毫秒，取决于具体存储技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。

## 6 集群成员变化

到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。

为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个leader同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性自动的转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图 10）。

![图 10](./images/raft-图10.png)

> 图 10：直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的leader在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。

为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理client请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合：

* log 被复制给集群中新、老配置的所有服务器。
* 新、旧配置的服务器都可以成为leader。
* 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。

共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程中依然响应client的请求。

集群配置在复制log中以特殊的log 来存储和通信；图 11 展示了配置转换的过程。当一个leader接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的log 和副本的形式。一旦一个服务器将新的配置log 增加到它的log中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着leader要使用  C-old,new 的规则来决定log  C-old,new 什么时候需要被提交。如果leader崩溃了，被选出来的新leader可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的candidate是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。

一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且leader完全特性保证了只有拥有 C-old,new log 的服务器才有可能被选举为leader。这个时候，leader创建一条关于 C-new 配置的log 并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。

![图 11](./images/raft-图11.png)

> 图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的配置log ，实线表示最后被提交的配置log 。leader首先创建了 C-old,new 的配置log在自己的log中，并提交到 C-old,new 中（C-old 的大多数和  C-new 的大多数）。然后他创建 C-new log并提交到 C-new 中的大多数。这样就不存在  C-new 和 C-old 可以同时做出决定的时间点。

在关于重新配置还有三个问题需要提出。第一个问题是，新的服务器可能初始化没有存储任何的log 。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的log 。为了避免这种可用性的间隔时间，Raft 在配置更新之前使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（leader复制log给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。

第二个问题是，集群的leader可能不是新配置的一员。在这种情况下，leader就会在提交了 C-new log之后退位（回到follower状态）。这意味着有这样的一段时间，leader管理着集群，但是不包括他自己；他复制log但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生leader过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的leader）。在此之前，可能只能从 C-old 中选出leader。

第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的RequestVote RPC，这样会导致当前的leader回退成follower状态。新的leader最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。

为了避免这个问题，当服务器确认当前leader存在时，服务器会忽略RequestVote RPC。特别的，当服务器在当前最小选举超时时间内收到一个RequestVote RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果leader能够发送心跳给集群，那么他就不会被更大的任期号废黜。

## 7 log压缩

Raft 的log在正常操作中不断的增长，但是在实际的系统中，log不能无限制的增长。随着log不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除log里积累的陈旧的信息，那么会带来可用性问题。

快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的log全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。

增量压缩的方法，例如log清理或者log结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是log清除方法就需要修改 Raft 了。

![图 12](./images/raft-图12.png)

> 图 12：一个服务器用新的快照替换了从 1 到 5 的log，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。

图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的log。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：**最后被包含索引**指的是被快照取代的最后的log在日志列表中的索引值（状态机最后应用的log项），**最后被包含的任期**指的是该log的任期号。保留这些数据是为了支持快照后紧接着的第一个log的附加log请求时的一致性检查，因为这个log需要前一log 的索引值和任期号。为了支持集群成员更新（第 6 节），快照中也将最后的一次配置作为最后一个log存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有log和快照了。

尽管通常服务器都是独立的创建快照，但是leader必须偶尔的发送快照给一些落后的follower。这通常发生在当leader已经丢弃了下一条需要发送给follower的log 的时候。幸运的是这种情况不是常规操作：一个与leader保持同步的follower通常都会有这个log。然而一个运行非常缓慢的follower或者新加入集群的服务器（第 6 节）将不会有这个log。这时让这个follower更新到最新的状态的方式就是通过网络把快照发送给他们。

**安装快照 RPC**：

由leader调用以将快照的分块发送给follower。leader总是按顺序发送分块。

| 参数 | 解释 |
|----|----|
| term | leader的任期号 |
| leaderId | leader的 Id，以便于follower重定向请求 |
| lastIncludedIndex | 快照中包含的最后log 的索引值 |
| lastIncludedTerm | 快照中包含的最后log 的任期号 |
| offset | 分块在快照中的字节偏移量 |
| data[] | 从偏移量开始的快照分块的原始字节 |
| done | 如果这是最后一个分块则为 true |

| 结果 | 解释 |
|----|----|
| term | 当前任期号（currentTerm），便于leader更新自己 |

**接收者实现**：

1. 如果`term < currentTerm`就立即回复
2. 如果是第一个分块（offset 为 0）就创建一个新的快照
3. 在指定偏移量写入数据
4. 如果 done 是 false，则继续等待更多的数据
5. 保存快照文件，丢弃具有较小索引的任何现有或部分快照
6. 如果现存的log 与快照中最后包含的log 具有相同的索引值和任期号，则保留其后的log 并进行回复
7. 丢弃整个log
8. 使用快照重置状态机（并加载快照的集群配置）

![图 13 ](./images/raft-图13.png)

> 图 13：一个关于安装快照的简要概述。为了便于传输，快照都是被分成分块的；每个分块都给了follower生命的迹象，所以follower可以重置选举超时计时器。


在这种情况下leader使用InstallSnapshot RPC 来发送快照给太落后的follower；见图 13。当follower通过这种  RPC 接收到快照时，他必须自己决定对于已经存在的log该如何处理。通常快照会包含没有在接收者log中存在的信息。在这种情况下，follower丢弃其整个log；它全部被快照取代，并且可能包含与快照冲突的未提交log。如果接收到的快照是自己log的前面部分（由于网络重传或者错误），那么被快照包含的log将会被全部删除，但是快照后面的log仍然有效，必须保留。

这种快照的方式背离了 Raft 的强leader原则，因为follower可以在不知道leader情况下创建快照。但是我们认为这种背离是值得的。leader的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有leader也是可以的。数据依然是从leader传给follower，只是follower可以重新组织他们的数据了。

我们考虑过一种替代的基于leader的快照方案，即只有leader创建快照，然后发送给所有的follower。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个follower都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，leader的实现会更加复杂。例如，leader需要发送快照的同时并行的将新的log 发送给follower，这样才不会阻塞新的client请求。

还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从log重建的时间。一个简单的策略就是当log大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。

第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。

## 8 client交互

这一节将介绍client是如何和 Raft 进行交互的，包括client如何发现leader和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。

Raft 中的client发送所有请求给leader。当client启动的时候，他会随机挑选一个服务器进行通信。如果client第一次挑选的服务器不是leader，那么那个服务器会拒绝client的请求并且提供他最近接收到的leader的信息（附加log请求包含了leader的网络地址）。如果leader已经崩溃了，那么client的请求就会超时；client之后会再次重试随机挑选服务器的过程。

我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft 是可以执行同一条命令多次的：例如，如果leader在提交了这条log之后，但是在响应client之前崩溃了，那么client会和新的leader重试这条指令，导致这条命令就被再次执行了。解决方案就是client对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。

只读的操作可以直接处理而不需要记录log。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为leader响应client请求时可能已经被新的leader作废了，但是他还不知道。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用log的情况下保证这一点。首先，leader必须有关于被提交log的最新信息。leader完全特性保证了leader一定拥有所有已经被提交的log ，但是在他任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条log 。Raft 中通过leader在任期开始的时候提交一个空白的没有任何操作的log 到log中去来实现。第二，leader在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的leader被选举出来）。Raft 中通过让leader在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，leader可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。

## 9 算法实现和评估

我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。

这一节会从三个方面来评估 Raft 算法：可理解性、正确性和性能。

### 9.1 可理解性

为了和 Paxos 比较 Raft 算法的可理解能力，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了log压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如leader选举）。小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者从第一部分的算法学习中获得的表现和经验的差异。我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。

我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些  Paxos 的经验，并且 Paxos 的视频要长 14%。如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。

| 关心 | 缓和偏见采取的手段 | 可供查看的材料 |
|----|----|----|
| 相同的讲课质量 | 两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。 | 视频 |
| 相同的测验难度 | 问题以难度分组，在两个测验里成对出现。| 小测验 |
| 公平评分 | 使用评价量规。随机顺序打分，两个测验交替进行。 | 评价量规（rubric） |

> 表 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。

参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图 14 展示了每个参与者的得分。配置t-检验（又称student‘s t-test）表明，在 95% 的可信度下，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。

![图 14](./images/raft-图14.png)

> 图 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。

我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型预测，对小测验的选择会产生 12.5 分的差别。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进行 Paxos 小测验的人而言，Raft的得分低了6.3分; 虽然我们不知道为什么，这似乎在统计上是有意义的。

我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。

![图 15](./images/raft-图15.png)

> 图 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。

关于 Raft 用户学习有一个更加详细的讨论。

### 9.2 正确性

在第 5 节，我们已经制定了正式的规范，和对一致性机制的安全性证明。这个正式规范使用 TLA+ 规范语言使图 2 中总结的信息非常清晰。它长约400行，并作为证明的主题。同时对于任何想实现 Raft 的人也是十分有用的。我们通过 TLA 证明系统非常机械的证明了log完全特性。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明规范的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性是完备的，并且是相当清晰的（大约 3500 个词）。

### 9.3 性能

Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当leader被选举成功时，什么时候复制新的log 。Raft 通过很少数量的消息包（一轮从leader到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。

我们使用我们自己的 Raft 实现来衡量 Raft leader选举的性能并且回答两个问题。首先，leader选举的过程收敛是否快速？第二，在leader宕机之后，最小的系统宕机时间是多久？

![图 16](./images/raft-图16.png)

> 图 16：发现并替换一个已经崩溃的leader的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小选举超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。

为了衡量leader选举，我们反复的使一个拥有五个节点的服务器集群的leader宕机，并计算需要多久才能发现leader已经宕机并选出一个新的leader（见图 16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的log，意味着有些candidate是没有成为leader的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止leader之前同步的发送了一次心跳广播（这大约和leader在崩溃前复制一个新的log给其他机器很像）。leader均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。

图 16 中上面的图表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，由于太多的选票瓜分的情况，选举过程往往都需要花费超过 10 秒钟。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。

图 16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的leader（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新leader之前，leader就很难发送完心跳包。这会导致没有意义的leader改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的leader改变，而且依然提供不错的可用性。

## 10 相关工作

已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中：

* Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。
* 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。
* 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。
* Paxos 可以应用的性能优化。
* Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于leader的方法，和 Raft 有很多相似之处。

Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用leader选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了leader身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，leader选举和基本的一致性协议是正交的：leader选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对leader选举的独立的机制。相比较而言，Raft 就直接将leader选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。

像 Raft 一样，VR 和 ZooKeeper 也是基于leader的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非leader的功能。例如，Raft 中log 都遵循着从leader发送给其他人这一个方向：AppendEntries RPC 是向外发送的。在 VR 中，log 的流动是双向的（leader可以在选举过程中接收log）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的log 也是双向传输的，但是它的实现更像 Raft。

和上述我们提及的其他基于一致性的log复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了log压缩和client交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 种消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在leader改变时传输了整个log；所以为了能够实践中使用，额外的消息类型就很必要了。

Raft 的强leader模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有leader的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。

一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有leader的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。

## 11 结论

算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。

在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；随着设计的进展，我们发现自己重复使用了一些技术，比如分解问题和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。

## 12 感谢

这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie\`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的leader Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。

## 参考

略
